# malloc_simulator
An (extremely basic) malloc simulator in Java. Done in collaboration with sjw103.

This simulator models requests which are made for a block of memory of some fixed number of pages. These blocks are then allocated into main memory, represented by a contiguous series of pages, from where they can be de-allocated at a random time. Sections of free pages are tracked by the free list, and defragmentation is automatically triggered whenever it exceeds a certain threshold.

## Implementation details
In ``Requests.java``, memory blocks are stored as an array, with each element of the array storing the size of each block (in pages). There is another array of the same size which keeps track of whether each block is currently requested or not, which prevents two separate processes from requesting the same block. One consequence of this is that there can only be one instance of each block in main memory (which is definitely intended).

The probability of freeing a memory block is stored as an array, with each element of this array corresponding to the probability of freeing the ith memory block. The probability of allocating a memory block is given by a 2D array, which makes use of the "me too" feature. Since a memory block's chances of being allocated can be dependent on a previous allocated block, this 2D array is supposed to represent this conditional probability. In this particular simulator, two blocks that are near to each other are much more likely to be allocated together rather than blocks that are farther away. I also picked this probability to be based on the most recent block that was allocated, though this can be interpreted differently in other simulators. And for the regular, independent probabilities, I just put them on the main diagonal of this matrix.

The probabilities themselves are simply half-Gaussian distribution. The independent probabilities have a standard deviation of 0.001, and their mean probabilities were close to 0.0007979. For the conditional probabilities, I took a standard deviation of 0.25 and raised it to the power of the difference between the indexes of the two blocks. This made closer blocks have a greater joint probability as a result. This also gave a small, but not trivial chance of the probability exceeding 1. However, this isn't really an issue as this would just be treated as having a probability of 1, as every value produced by ``Math.random()`` falls between 0 and 1.

In ``Simulator.java``, the free list is tracked using an ArrayList of two-element arrays, holding the start and end indexes of a contiguous space of free memory. Though somewhat clunky, this method was still fairly convenient and also seemed to be relatively space-efficient, as opposed to actually trying to store an array that represented all of memory space. HashMaps were used to associate a specific block with the first page it occupies in memory. Since you can easily get the block's size using its identifying number, this is all that was needed to associate a block with its place in memory.

Percent fragmentation was calculated by the ratio between the size of the longest contiguous free memory block and the total amount of memory available, subtracted from 1. In effect, if most of the memory available was all in a single chunk, the amount of fragmentation would be low. However, as free memory is more distributed in smaller bits throughout the memory space, the amount of fragmentation increases.

And finally, defragmentation was a fairly straightforward process. After the percent fragmentation reaches some threshold, all of the memory that's being used by processes would be shifted toward the beginning of the heap. As they coalesce, unused memory would combine until the entire memory heap became a single block of used memory, followed by a single block of free memory. Every block that is shifted is tallied as part of the defragmentation overhead, and the free list and hash map needed to be updated accordingly in order to account for the changes.
